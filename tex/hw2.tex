\documentclass{article}

\input{structure.tex} % Include the file specifying the document structure and custom commands

\title{Probability and Stochastic Processes (1) }
\author{Yu Yangcheng, 2023010719\\ Tsinghua University}
\date{\textbf{Feb. 27, 2025}}

\begin{document}

\maketitle

\section*{Problem Set 2}


\begin{question}
You enter a special kind of chess tournament, in which you play one game with each of three opponents, but you get to choose the order in which you play your opponents, knowing the probability of a win against each. You win the tournament if you win two games in a row, and you want to maximize the probability of winning. Show that it is optimal to play the weakest opponent second, and that the order of playing the other two opponents does not matter.
\end{question}

    \textbf{Proof.}    
    Let the order of the game be a, b, and c, then the probability of winning the tournament is $$ P(a)P(b)+P(b)P(c)-P(a)P(b)P(c) ,$$
    where \( P(a) \) is the probability of winning the first game, and so on. Obviously, we have $P(a)P(b)P(c)=constant$, so we need to proof that \( P(a)P(b) +P(b)P(c) \) is maximized when b is the weakest opponent. Since \(P(a)P(b)+P(a)P(c)+P(b)P(c)\), that's equivalent to proof that \( P(a)P(c) \) is minimized when b is the weakest opponent. That's trivial since $P(a)$, $P(b)$ are the smallest two probabilities. Additionally, the order of playing the other two opponents doesn't change the value of $P(a)P(c)$. Therefore, the statement is true.


\begin{question}
Show that a countable intersection of events with probability 1 still has probability 1.
\end{question}

    \textbf{Proof.}
    Let's consider the sequence of events $A_1,A_2,\dots $ such that 
    $$ P(A_1)=P(A_2)=\dots=1 $$
    Consider the implement of intersections of $A_i$, $i.e.$
    $$ (\bigcap_{i=1}^{\infty} A_i)^c=\bigcup _{i=1}^{\infty}A_i^c $$
    Given the possibility of event $A_i$ is 1, we have 
    $$ P(\bigcup _{i=1}^{\infty}A_i^c)\leq \sum_{i=1}^{\infty}P(A_i^c)=\sum_{i=1}^{\infty}0=0 $$
    Therefore, 
    $$ P(\bigcap_{i=1}^{\infty} A_i)=1-P((\bigcap_{i=1}^{\infty})^c)=1 $$

\begin{question}
We are given three coins: one has heads on both faces, the second has tails on both faces, and the third has a head on one face and a tail on the other. We choose a coin at random, toss it, and the result is heads. What is the probability that the opposite face is tails?
\end{question}

    \textbf{Proof.}
    Let A = The result is head, B = The opposite face is tails. Then 
    $$ P(A)=\frac{1}{3}\times 1+\frac{1}{3}\times 0+\frac{1}{3}\times \frac{1}{2}=\frac{1}{2} $$ 
    And 
    $$ P(AB)=\frac{1}{3} $$
    Under Bayes's theorem, 
    $$ P(B|A)=\frac{P(AB)}{P(A)}=\frac{2}{3} $$
    That's the probability that the opposite face is tails.

\begin{question}
Each of \(k\) jars contains \(m\) white and \(n\) black balls. A ball is randomly chosen from jar 1 and transferred to jar 2, then a ball is randomly chosen from jar 2 and transferred to jar 3, etc. Finally, a ball is randomly chosen from jar \(k\). Show that the probability that the last ball is white is the same as the probability that the first ball is white, i.e., it is \( \frac{m}{m + n} \).
\end{question}

    \textbf{Proof.}
    Let P(the i th. ball is white)=$P_i$. We should prove that $P_i\equiv \frac{m}{m+n}$. We verify the base case, when i = 1, the statement is obviously right. Assume the statement holds for $i=p$, that is, 
    $$ P(\text{the p th. ball is white})=\frac{m}{m+n} $$
    Now, we prove that the statement holds for p+1: 
    $$ P_{p+1}=P_p\frac{m+1}{n+m+1}+(1-P_p) \frac{m}{n+m+1}= \frac{m}{m+n}\frac{m+1}{m+n+1}+\frac{n}{m+n}\frac{m}{m+n+1}=\frac{m}{m+n}$$
    The proposition is proved by mathematical induction.

\begin{question}
Suppose we would like to represent an infinite sequence of binary observations, where each observation is a zero or one with equal probability. For example, the experiment could consist of repeatedly flipping a fair coin, and recording a one each time it shows heads and a zero each time it shows tails. Then an outcome \( \omega \) would be an infinite sequence, \( \omega = (\omega_1, \omega_2, ...) \), such that for each \(i \geq 1\), \( \omega_i \in \{0, 1\} \). Let \( \Omega \) be the set of all such \( \omega \)â€™s. The associated \( \sigma \)-algebra \( \mathcal{F} \) satisfies that any set that can be defined in terms of finitely many of the observations is in \( \mathcal{F} \). In particular, for any binary sequence \( (b_1, b_2, ..., b_k) \) of some finite length \( k \), the set \( \{ \omega \in \Omega : w_i = b_i \text{ for } 1 \leq i \leq k \} \) should be in \( \mathcal{F} \) with a probability of \( 2^{-k} \). Suppose that there are two players who take turns performing the coin flips, with the first one to get heads wins. Let \( F \) be the event that the player going first wins. Show that \( F \in \mathcal{F} \) and find \( P(F) \).
\end{question}

    \textbf{Proof.}
    \begin{enumerate}
        \item \( F \in \mathcal{F} \) \\
        We should show that the event that the first player wins can be represented by a countable union of sets in \( \mathcal{F} \). The player goes first wins could happen in the following sequence: 1, 001, 00001... Therefore, F can be expressed by the union of these events. By hypothesis, these events are included in \( \mathcal{F} \). Therefore, \( F \in \mathcal{F} \).
        \item find \( P(F) \) \\
        According to the discussion above, we have 
        $$ P(F)=P(1)+P(001)+P(00001)+\dots=\frac{1}{2}+\frac{1}{8}+\dots=\sum_{k=1}^{\infty}\frac{1}{2^{2k-1}}=\frac{2}{3} $$
    \end{enumerate}
    That's the final answer.

\end{document}
